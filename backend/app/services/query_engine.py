"""Query engine for the AI chat dataviz layer.

Receives a declarative query DSL (generated by the LLM), validates it
against the metamodel, compiles it to SQLAlchemy, injects the security
context (user_id + account_ids), executes, and returns flat data rows.

Security model:
  - user_id and account_ids are ALWAYS injected by the engine (never from LLM)
  - the LLM can filter by account attributes (name, type) but this can only
    *narrow* the scope — the WHERE account_id IN (...) is always applied
  - soft-deleted rows are always excluded
"""

from __future__ import annotations

import re
from dataclasses import dataclass
from datetime import date
from decimal import Decimal

import structlog
from sqlalchemy import case, func, select
from sqlalchemy.dialects import postgresql
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.sql import Select

from app.models.account import Account
from app.models.category import Category
from app.models.transaction import Transaction
from app.services.metamodel import (
    ALL_SOURCES,
    ALLOWED_AGGREGATES,
    ALLOWED_FILTER_OPS,
    ALLOWED_TEMPORAL_FUNCTIONS,
    FieldType,
)

logger = structlog.get_logger()

MAX_RESULT_ROWS = 1000


# ---------------------------------------------------------------------------
# Query context — provided by the application, NEVER by the LLM
# ---------------------------------------------------------------------------

@dataclass
class QueryContext:
    """Security/scope context injected into every query."""
    user_id: int
    account_ids: list[int]  # accounts selected in the UI (scope ceiling)


# ---------------------------------------------------------------------------
# DSL types (parsed from LLM JSON)
# ---------------------------------------------------------------------------

@dataclass
class FilterClause:
    field: str       # e.g. "date", "category.name", "account.type"
    op: str          # e.g. "=", ">=", "in"
    value: object    # scalar or list

@dataclass
class AggregateClause:
    fn: str          # e.g. "sum", "count"
    field: str | None = None  # None for count(*)
    alias: str | None = None  # output column name

@dataclass
class OrderByClause:
    field: str
    dir: str = "desc"

@dataclass
class DatavizQuery:
    """Parsed query DSL from LLM."""
    source: str
    filters: list[FilterClause]
    group_by: list[str]
    aggregates: list[AggregateClause]
    order_by: list[OrderByClause]
    limit: int | None = None


# ---------------------------------------------------------------------------
# Parser — JSON dict → DatavizQuery
# ---------------------------------------------------------------------------

class QueryValidationError(Exception):
    """Raised when the LLM query DSL is invalid."""


class QueryExecutionError(Exception):
    """Raised when a validated query fails at execution time (SQL error).

    Carries the compiled SQL and original query DSL so the debug layer
    can display exactly what went wrong.
    """

    def __init__(self, message: str, *, sql: str | None = None, query_dsl: dict | None = None):
        super().__init__(message)
        self.sql = sql
        self.query_dsl = query_dsl


def parse_query(raw: dict) -> DatavizQuery:
    """Parse and validate a raw JSON query dict into a DatavizQuery."""
    source = raw.get("source")
    if not source or source not in ALL_SOURCES:
        raise QueryValidationError(f"Source inconnue : {source!r}. Sources valides : {list(ALL_SOURCES)}")

    source_def = ALL_SOURCES[source]

    # --- filters ---
    filters = []
    for f in raw.get("filters", []):
        field_name = f.get("field", "")
        op = f.get("op", "=")
        value = f.get("value")
        _validate_field_ref(field_name, source_def)
        if op not in ALLOWED_FILTER_OPS:
            raise QueryValidationError(f"Opérateur invalide : {op!r}")
        filters.append(FilterClause(field=field_name, op=op, value=value))

    # --- group_by ---
    group_by = []
    for g in raw.get("groupBy", raw.get("group_by", [])):
        _validate_group_ref(g, source_def)
        group_by.append(g)

    # --- aggregates ---
    aggregates = []
    for a in raw.get("aggregates", []):
        fn = a.get("fn", "")
        if fn not in ALLOWED_AGGREGATES:
            raise QueryValidationError(f"Fonction d'agrégation invalide : {fn!r}")
        agg_field = a.get("field")
        if agg_field and not _is_temporal_call(agg_field):
            _validate_field_ref(agg_field, source_def)
        alias = a.get("as") or a.get("alias") or f"{fn}_{agg_field or 'all'}"
        aggregates.append(AggregateClause(fn=fn, field=agg_field, alias=alias))

    # --- orderBy ---
    order_by = []
    for o in raw.get("orderBy", raw.get("order_by", [])):
        order_by.append(OrderByClause(
            field=o.get("field", ""),
            dir=o.get("dir", "desc"),
        ))

    limit = raw.get("limit")
    if limit is not None:
        limit = min(int(limit), MAX_RESULT_ROWS)

    return DatavizQuery(
        source=source,
        filters=filters,
        group_by=group_by,
        aggregates=aggregates,
        order_by=order_by,
        limit=limit,
    )


# ---------------------------------------------------------------------------
# Validation helpers
# ---------------------------------------------------------------------------

_TEMPORAL_RE = re.compile(r"^(month|quarter|year|week|day)\((\w+)\)$")


def _is_temporal_call(ref: str) -> bool:
    return bool(_TEMPORAL_RE.match(ref))


def _parse_temporal_call(ref: str) -> tuple[str, str] | None:
    m = _TEMPORAL_RE.match(ref)
    if m:
        return m.group(1), m.group(2)
    return None


def _validate_field_ref(ref: str, source_def) -> None:
    """Validate a field reference like 'amount', 'category.name', or 'month(date)'."""
    # Temporal function
    parsed = _parse_temporal_call(ref)
    if parsed:
        fn_name, inner_field = parsed
        if fn_name not in ALLOWED_TEMPORAL_FUNCTIONS:
            raise QueryValidationError(f"Fonction temporelle invalide : {fn_name}")
        _validate_field_ref(inner_field, source_def)
        return

    # Dotted reference (relation.field)
    if "." in ref:
        parts = ref.split(".", 1)
        relation_name, field_name = parts
        if relation_name not in source_def.relation_targets():
            raise QueryValidationError(
                f"Relation {relation_name!r} non disponible sur {source_def.name!r}"
            )
        rel_source = ALL_SOURCES.get(relation_name)
        if not rel_source:
            raise QueryValidationError(f"Source {relation_name!r} inconnue")
        if field_name not in rel_source.field_names():
            raise QueryValidationError(
                f"Champ {field_name!r} inconnu sur {relation_name!r}. "
                f"Champs valides : {sorted(rel_source.field_names())}"
            )
        return

    # Direct field
    if ref == "direction":
        return  # virtual field, always valid on transactions
    if ref not in source_def.field_names():
        raise QueryValidationError(
            f"Champ {ref!r} inconnu sur {source_def.name!r}. "
            f"Champs valides : {sorted(source_def.field_names())}"
        )


def _validate_group_ref(ref: str, source_def) -> None:
    """groupBy entries can be field refs or temporal calls."""
    _validate_field_ref(ref, source_def)


# ---------------------------------------------------------------------------
# Compiler — DatavizQuery → SQLAlchemy Select
# ---------------------------------------------------------------------------

def _resolve_column(ref: str, source_name: str):
    """Resolve a field reference to a SQLAlchemy column expression."""
    # Temporal function
    parsed = _parse_temporal_call(ref)
    if parsed:
        fn_name, inner_field = parsed
        inner_col = _resolve_column(inner_field, source_name)
        return func.date_trunc(fn_name, inner_col)

    # Direction virtual field
    if ref == "direction":
        return case(
            (Transaction.amount > 0, "income"),
            else_="expense",
        )

    # Dotted (relation)
    if "." in ref:
        relation, field_name = ref.split(".", 1)
        model = _relation_model(relation)
        return getattr(model, field_name)

    # Direct field on primary source
    model = _source_model(source_name)
    # Map metamodel field names to actual model columns
    col_map = _column_map(source_name)
    actual_col = col_map.get(ref, ref)
    return getattr(model, actual_col)


def _source_model(source_name: str):
    return {"transactions": Transaction, "category": Category, "account": Account}[source_name]


def _relation_model(relation: str):
    return {"category": Category, "account": Account}[relation]


def _column_map(source_name: str) -> dict[str, str]:
    """Map metamodel field names → actual SQLAlchemy column names."""
    if source_name == "transactions":
        return {
            "label": "label_clean",  # will be coalesced with label_raw
            "date": "date",
            "amount": "amount",
            "currency": "currency",
        }
    if source_name == "category":
        return {"parent_name": "parent_id"}  # handled specially
    return {}


def _label_column():
    """Effective label: prefer label_clean when set, else label_raw."""
    return func.coalesce(
        func.nullif(func.trim(Transaction.label_clean), ""),
        Transaction.label_raw,
    )


def _compile_select(query: DatavizQuery) -> tuple[Select, list[str]]:
    """Compile a DatavizQuery into a SQLAlchemy Select.

    Returns (statement, output_column_names).
    """
    columns = []
    col_names = []
    group_by_cols = []

    # GROUP BY columns first
    for g in query.group_by:
        col_expr = _resolve_select_column(g, query.source)
        label = _col_label(g)
        labeled = col_expr.label(label)
        columns.append(labeled)
        col_names.append(label)
        group_by_cols.append(col_expr)

    # Aggregate columns
    for agg in query.aggregates:
        agg_expr = _compile_aggregate(agg, query.source)
        columns.append(agg_expr.label(agg.alias))
        col_names.append(agg.alias)

    # If no group_by and no aggregates, select raw fields
    if not query.group_by and not query.aggregates:
        # Select useful default columns
        defaults = _default_columns(query.source)
        for label, col in defaults:
            columns.append(col.label(label))
            col_names.append(label)

    # Build FROM clause
    stmt = select(*columns).select_from(Transaction)

    # Joins
    needs_category = _needs_relation(query, "category")
    needs_account = _needs_relation(query, "account")
    if needs_category:
        stmt = stmt.outerjoin(Category, Transaction.category_id == Category.id)
    if needs_account:
        stmt = stmt.outerjoin(Account, Transaction.account_id == Account.id)

    # WHERE: always exclude soft-deleted
    stmt = stmt.where(Transaction.deleted_at.is_(None))

    # WHERE: LLM filters
    for f in query.filters:
        stmt = _apply_filter(stmt, f, query.source)

    # GROUP BY
    if group_by_cols:
        stmt = stmt.group_by(*group_by_cols)

    # ORDER BY
    for o in query.order_by:
        col_expr = _resolve_order_column(o.field, query, col_names)
        if o.dir == "asc":
            stmt = stmt.order_by(col_expr.asc())
        else:
            stmt = stmt.order_by(col_expr.desc())

    # LIMIT
    limit = query.limit or MAX_RESULT_ROWS
    stmt = stmt.limit(min(limit, MAX_RESULT_ROWS))

    return stmt, col_names


def _resolve_select_column(ref: str, source_name: str):
    """Like _resolve_column but handles 'label' with coalesce."""
    if ref == "label" and source_name == "transactions":
        return _label_column()
    if ref == "category.parent_name":
        # Special: need to self-join to get parent name
        # For simplicity, use a subquery-style approach
        # Actually, Category has parent relationship, but for SQL we
        # need a separate alias. For now return parent_id and handle post-query.
        # Let's use a scalar subquery instead:
        from sqlalchemy import literal_column
        parent_cat = Category.__table__.alias("parent_cat")
        return (
            select(parent_cat.c.name)
            .where(parent_cat.c.id == Category.parent_id)
            .correlate(Category)
            .scalar_subquery()
        )
    return _resolve_column(ref, source_name)


def _compile_aggregate(agg: AggregateClause, source_name: str):
    """Compile an aggregate clause to a SQLAlchemy expression."""
    if agg.fn == "count" and not agg.field:
        return func.count()
    if agg.fn == "count_distinct":
        col = _resolve_select_column(agg.field, source_name) if agg.field else Transaction.id
        return func.count(func.distinct(col))

    col = _resolve_select_column(agg.field, source_name) if agg.field else Transaction.id
    fn_map = {
        "sum": func.sum,
        "count": func.count,
        "avg": func.avg,
        "min": func.min,
        "max": func.max,
    }
    return fn_map[agg.fn](col)


def _apply_filter(stmt: Select, f: FilterClause, source_name: str) -> Select:
    """Apply a single filter clause to the statement."""
    col = _resolve_select_column(f.field, source_name)

    # Direction virtual field
    if f.field == "direction":
        if f.op == "=" and f.value == "income":
            return stmt.where(Transaction.amount > 0)
        elif f.op == "=" and f.value == "expense":
            return stmt.where(Transaction.amount < 0)
        return stmt

    value = _coerce_value(f.value, f.field)

    if f.op == "=":
        return stmt.where(col == value)
    elif f.op == "!=":
        return stmt.where(col != value)
    elif f.op == ">":
        return stmt.where(col > value)
    elif f.op == "<":
        return stmt.where(col < value)
    elif f.op == ">=":
        return stmt.where(col >= value)
    elif f.op == "<=":
        return stmt.where(col <= value)
    elif f.op == "in":
        if isinstance(value, list):
            return stmt.where(col.in_(value))
    elif f.op == "not_in":
        if isinstance(value, list):
            return stmt.where(col.notin_(value))
    elif f.op == "like":
        return stmt.where(col.ilike(f"%{value}%"))
    return stmt


def _coerce_value(value, field_ref: str):
    """Coerce filter values to proper Python types."""
    if isinstance(value, str):
        # Try date
        if re.match(r"^\d{4}-\d{2}-\d{2}$", value):
            return date.fromisoformat(value)
    return value


def _resolve_order_column(ref: str, query: DatavizQuery, col_names: list[str]):
    """Resolve an orderBy field — could be an alias or a column ref."""
    # Check if it matches an aggregate alias
    for agg in query.aggregates:
        if agg.alias == ref:
            return _compile_aggregate(agg, query.source)
    # Check group_by labels
    for g in query.group_by:
        if _col_label(g) == ref:
            return _resolve_select_column(g, query.source)
    # Fall back to column
    return _resolve_select_column(ref, query.source)


def _col_label(ref: str) -> str:
    """Generate a column label from a field reference."""
    # month(date) → month
    parsed = _parse_temporal_call(ref)
    if parsed:
        return parsed[0]
    # category.name → category_name
    return ref.replace(".", "_")


def _needs_relation(query: DatavizQuery, relation: str) -> bool:
    """Check if query references a relation."""
    refs = []
    refs.extend(f.field for f in query.filters)
    refs.extend(query.group_by)
    refs.extend(a.field for a in query.aggregates if a.field)
    refs.extend(o.field for o in query.order_by)
    return any(r.startswith(f"{relation}.") for r in refs)


def _default_columns(source_name: str) -> list[tuple[str, object]]:
    """Default columns when no groupBy/aggregates specified."""
    return [
        ("date", Transaction.date),
        ("label", _label_column()),
        ("amount", Transaction.amount),
        ("category_name", Category.name),
    ]


# ---------------------------------------------------------------------------
# Executor
# ---------------------------------------------------------------------------

def _compile_sql_text(stmt: Select) -> str:
    """Compile a SQLAlchemy Select to a readable SQL string (for debug)."""
    try:
        compiled = stmt.compile(
            dialect=postgresql.dialect(),
            compile_kwargs={"literal_binds": True},
        )
        return str(compiled)
    except Exception:
        # Fallback: without literal binds (params shown as placeholders)
        try:
            return str(stmt.compile(dialect=postgresql.dialect()))
        except Exception:
            return "<unable to compile>"


async def execute_query(
    db: AsyncSession,
    raw_query: dict,
    context: QueryContext,
) -> tuple[list[dict], list[str], str | None]:
    """Parse, validate, compile and execute a dataviz query.

    Returns:
        (rows, column_names, sql_text) where rows is a list of dicts,
        and sql_text is the compiled SQL for debug purposes.

    Raises:
        QueryValidationError on invalid DSL.
    """
    query = parse_query(raw_query)

    # Handle virtual sources
    if query.source == "balance":
        data, col_names = await _execute_balance_query(db, query, context)
        return data, col_names, "-- virtual source: balance (no SQL)"

    try:
        stmt, col_names = _compile_select(query)
    except Exception as exc:
        raise QueryExecutionError(
            f"Erreur de compilation : {exc}",
            sql=None,
            query_dsl=raw_query,
        ) from exc

    # ---- SECURITY: inject context (always) ----
    stmt = stmt.where(Transaction.account_id.in_(context.account_ids))

    # Capture SQL text for debug before executing
    sql_text = _compile_sql_text(stmt)

    logger.debug("query_engine_execute", source=query.source, col_names=col_names)

    try:
        result = await db.execute(stmt)
    except Exception as exc:
        # Roll back the failed statement so the session stays usable
        await db.rollback()
        raise QueryExecutionError(
            f"Erreur SQL : {exc}",
            sql=sql_text,
            query_dsl=raw_query,
        ) from exc

    rows = result.all()

    # Convert to list of dicts
    data = []
    for row in rows:
        d = {}
        for i, name in enumerate(col_names):
            val = row[i]
            # Serialisation
            if isinstance(val, Decimal):
                val = float(val)
            elif isinstance(val, date):
                val = val.isoformat()
            elif hasattr(val, "isoformat"):
                # datetime
                val = val.strftime("%Y-%m")
            d[name] = val
        data.append(d)

    return data, col_names, sql_text


async def _execute_balance_query(
    db: AsyncSession,
    query: DatavizQuery,
    context: QueryContext,
) -> tuple[list[dict], list[str]]:
    """Execute a balance virtual source query.

    Computes current balance per account (initial_balance + sum of transactions).
    """
    # Base: accounts in scope
    acct_query = (
        select(
            Account.id,
            Account.name.label("account_name"),
            Account.initial_balance,
        )
        .where(
            Account.id.in_(context.account_ids),
            Account.status == "active",
        )
    )
    acct_result = await db.execute(acct_query)
    accounts = acct_result.all()

    data = []
    col_names = ["account_name", "amount"]
    today = date.today()

    for acct in accounts:
        txn_sum_result = await db.execute(
            select(func.coalesce(func.sum(Transaction.amount), 0)).where(
                Transaction.account_id == acct.id,
                Transaction.date <= today,
                Transaction.deleted_at.is_(None),
            )
        )
        txn_sum = txn_sum_result.scalar() or Decimal("0")
        balance = float(acct.initial_balance + txn_sum)
        data.append({
            "account_name": acct.account_name,
            "amount": balance,
        })

    # Apply LLM filters (account.name, account.type) — post-filter
    for f in query.filters:
        if f.field in ("account_name", "account.name"):
            data = [d for d in data if d["account_name"] == f.value]

    # Total row if aggregated
    if any(a.fn == "sum" for a in query.aggregates):
        total = sum(d["amount"] for d in data)
        return [{"amount": total}], ["amount"]

    return data, col_names
